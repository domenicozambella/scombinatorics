% !TEX root = scombinatorics.tex
\documentclass[scombinatorics.tex]{subfiles}
\begin{document}
\chapter{Law(s) of large numbers}
\label{ulln}



\def\medrel#1{\parbox[t]{5ex}{$\displaystyle\hfil #1$}}
\def\ceq#1#2#3{\parbox[t]{20ex}{$\displaystyle #1$}\medrel{#2}{$\displaystyle #3$}}

Quoting from some unpublished notes by Carlos C.~Rodr\'iguez

\begin{quotation}\parindent0mm\parskip1ex\noindent
What is a Law of Large Numbers?
I am glad you asked! The Laws of Large Numbers, or LLNs for short, come in
three basic flavors: Weak, Strong and Uniform. They all state that the observed
frequencies of events tend to approach the actual probabilities as the number
of observations increases. Saying it in another way, the LLNs show that under
certain conditions, we can asymptotically learn the probabilities of events from
their observed frequencies. To add some drama we could say that if God is
not cheating and S/he doesnâ€™t change the initial standard probabilistic model
too much then, in principle, we (or other machines, or even the universe as a
whole) could eventually find out the Truth, the whole Truth, and nothing but
the Truth.

Bull! The Devil, is in the details.

I suspect that for reasons not too different in spirit to the ones above, famous
minds of the past took the slippery slope of defining probabilities as the limits
of relative frequencies. They became known as ``frequentist''. They wrote
books and indoctrinated generations of confused students.
\end{quotation}
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
\section{Inequalities}

Throughout this and the next section we work with a given probability space $\U,\Pr$.
For simplicity, the following two propositions are proved for discrete $\Pr$, but they are easily seen to hold in  general.

\begin{definition}\label{def_convex}
  A function $f:\RR\to\RR$ is \emph{convex\/} if for every tuples of real numbers $p_i$ and $x_i$ such that

  \ceq{\hfill\sum^n_{i=1}p_i}
  {=}
  {1}

  we have

  \ceq{\hfill f\bigg(\sum^n_{i=1}p_ix_i\bigg)}
  {\le}
  {\sum^n_{i=1}p_if(x_i).}\QED
\end{definition}

Note that, though the definition is usually given with $n=2$, the general property above follows easily.

\begin{void_thm}[Jensen's Inequality]\label{Jensen}
Let $f:\RR\to\RR$ be a convex function.
  Then

  \ceq{\hfill f\Big(\Ex[X]\Big)}
  {\le}
  {\Ex\Big[f(X)\Big]}
\end{void_thm}


\begin{proof}
  For simplicity, assume that the sample space $\U$ is finite.
  Then the claim is obvious from the definition.
\end{proof}

The following is arguably the most basic inequality in probability theory.
Although it is almost trivial, it will be required several times in this chapter. 

\begin{void_thm}[Markov's Inequality]\label{Markov}
  Let $X$ be a nonnegative random variable with finite mean.
  Then for every $\epsilon>0$

  \ceq{\hfill\Pr\Big(X\ge\epsilon\Big)}
  {\le}
  {\frac{\Ex[X]}{\epsilon}}
\end{void_thm}

\begin{proof}
  For simplicity, assume that the sample space $\U$ is finite.
  (The theorem holds in general, but we only need the finite case.)
  Define $A=\{a\in\U\,:\, X(a)\ge\epsilon\}$.

  \ceq{\hfill\Ex[X]}{=}{\sum_{a\in\U} \Pr(a)\, X(a)}

  \ceq{}{=}{\sum_{a\in A} \Pr(a)\, X(a) + \sum_{a\notin A} \Pr(a)\, X(a)}

  \ceq{}{\ge}{\sum_{a\in A} \Pr(a)\, X(a)}

  \ceq{}{\ge}{\epsilon\sum_{a\in A}\Pr(a)}

  \ceq{}{=}{\epsilon\,\Pr(X\ge\epsilon)}
\end{proof}

\begin{corollary}
  Let $X$ be a nonnegative random variable.
  If $\Ex\big[X^k\big]$ exists, then for every $\epsilon>0$
  
  \ceq{\hfill\Pr\Big(X\ge\epsilon\Big)}
  {\le}
  {\frac{\Ex[X^k]}{\epsilon^k}}
\end{corollary}

\begin{proof}
  By Markov's inequality, since $\Pr\big(X\ge\epsilon\big)=\Pr\big(X^k\ge\epsilon^k\big)$.
\end{proof}

Chebyshev's inequality (a.k.a. Chebysheff, Chebyshov, Tschebyscheff, Tschebycheff) is a special case of the corollary above.

\begin{void_thm}[Chebyshev's Inequality]\label{Chebyshev}
    Let $X$ be a random variable with finite mean and variance.
    Then for every $\epsilon>0$
    
    \ceq{\hfill\Pr\Big(\big|X-\Ex[X]\big|\ge\epsilon\Big)}
    {\le}
    {\frac{\Var[X]}{\epsilon^2}}\QED
\end{void_thm}

To obtain exponential bounds, we frequently apply the following trick.

\begin{void_thm}[Chernoff's method]\label{lem_chernoff_method}
  Let $X$ be a random variable with finite mean.
  Then for every $t>0$

  \ceq{\hfill\Pr\big(X\ge\epsilon\big)}
  {\le}
  {e^{-t\epsilon}\,\Ex\big[e^{tX}\big]}
\end{void_thm}

\begin{proof}
  For  every $t>0$

  \ceq{\hfill\Pr\big(X\ge\epsilon\big)}
  {=}
  {\Pr\big(e^{tX}\ge e^{t\epsilon}\big)}
  \hfill because $e^{tx}$ is increasing
  
  \ceq{}
  {\le}
  {e^{-t\epsilon}\,\Ex\big[e^{tX}\big],}

  by Markov's inequality, which me may apply since $e^{tX}$ is always positive. 
\end{proof}


  \begin{void_thm}[Hoeffding's lemma]\label{lem_Hoeffding}
    Let $X$ be a bounded random variable, say $a\le X\le b$. 
    Let $\Ex[X]=\mu$ and $d=b-a$.
    Then
    
    \ceq{\hfill\Ex\Big[e^{t(X-\mu)}\Big]}
    {\le}
    {\exp{\Big(\frac{t^2d^2}8}\Big).}\smallskip
  \end{void_thm}

\begin{proof}
  For clarity, assume $\mu=0$.
  The general result follows easily from this special case by centralization.
  Recall that, by convexity, for every $x\in[a,b]$

  \ceq{\hfill e^{tx}}
  {\le}
  {\frac{x-a}{d}\ e^{tb}\ +\ \frac{b-x}{d}\ e^{ta}}

  Then

  \ceq{\hfill e^{tX}}
  {\le}
  {\frac{X-a}{d}\ e^{tb}\ +\ \frac{b-X}{d}\ e^{ta}}

  By the linearity of expectation,

  \ceq{\hfill\Ex\Big[e^{tX}\Big]}
  {\le}
  {\frac{b\,e^{ta}-a\,e^{tb}}{d}}

  \ceq{\hfill\log\Ex\Big[e^{tX}\Big]}
  {\le}
  {\log\frac{b\,e^{ta}-a\,e^{tb}}{d}}

  taking the Taylor series expansion of the r.h.s.\@ at $t=0$ (the first and second derivatives vanish at $0$; the second derivative is always $\le d^2/4$) we obtain

  \ceq{\hfill\log\Ex\Big[e^{tX}\Big]}
  {\le}
  {\frac{t^2d^2}8.}
\end{proof}

\begin{void_thm}[Hoeffding's Inequality]\label{Hoeffding_inequality}
  Let $X_1,\dots,X_n$ be independent random variables with bounded range, say $a\le X_i\le b$. Define $d=b-a$.
  
  \ceq{\hfill M}{=}{\sum^n_{i=1}\Big(X_i-\Ex[X_i]\Big)}
  
  Then for every $\epsilon>0$ 

  \ceq{\hfill\Pr \Big(M\ge\epsilon\Big)}
  {\le}
  {\exp{\Big(-\frac{2\epsilon^2}{nd^2}}\Big),}

  \ceq{\hfill\Pr \Big(M\le-\epsilon\Big)}
  {\le}
  {\exp{\Big(-\frac{2\epsilon^2}{nd^2}}\Big).}\smallskip

  Clearly, the two inequalities above imply the following

\ceq{\hfill\Pr \Big(|M|\ge\epsilon\Big)}
    {\le}
    {2\exp{\Big(-\frac{2\epsilon^2}{nd^2}}\Big).}\smallskip

\end{void_thm}

\begin{proof}
  Define $\Ex[X_i]=\mu_i$.
  Let $t>0$ be arbitrary.\smallskip

  \ceq{\hfill \Pr\Big(M\ge\epsilon\Big)}{\le}{e^{-t\epsilon}\,\Ex\Big[e^{tM}\Big]}\hfill by Chernoff's method (\ref{lem_chernoff_method})

  \ceq{}
      {=}
      {e^{-t\epsilon}\,\prod^n_{i=1}\Ex\Big[e^{t\,(X_i-\mu_i)}\Big]}
      \hfill by independence.


      \ceq{}
      {\le}
      {e^{-t\epsilon}\,\prod^n_{i=1}\exp\Big(\frac{t^2d^2}{8}\Big)}
      \hfill by Hoeffding's Lemma (\ref{lem_Hoeffding}).

  \ceq{}
      {=}
      {\exp\Big(\frac{n\,t^2d^2}{8}-t\epsilon\Big)}

  Now substitute $4\epsilon/nd^2$ for $t$.
\end{proof}



We prove Hoeffding's lemma with a slightly weaker bound ($2$ for $8$).
The purpose is to present two clever tricks  \textit{ghost sample\/} and \textit{symmetrization\/} which in the following section is applied in a more complex setting.

First we need the following lemma.
A \emph{random sign variable\/} (a.k.a.\@ Rademacher random variable) is a random variable $\sigma\in\{-1,1\}$ with mean $0$.

\begin{lemma}\label{lem_sign}
  Let $\sigma$ be a random sign variable.
  Then for every $t$

  \ceq{\hfill\Ex\Big[e^{t\sigma}\Big]}
      {\le}
      {e^{t^2/2}}
\end{lemma}

\begin{proof}
  Replace $e^x$ with its Taylor expansion around $x=0$

  \ceq{\hfill\Ex\Big[e^{t\sigma}\Big]}
  {=}
  %     {\Ex\Bigg[\sum^\infty_{i=0}\frac{(t\sigma)^i}{i!}\Bigg]}
  %
  % \ceq{}
  %     {=}
  {\sum^\infty_{i=0}\frac{t^i\Ex\big[\sigma^i\big]}{i!}}

  \ceq{}
  {=}
  {\sum^\infty_{i=0}\frac{t^{2i}}{(2i)!}}
  \hfill since
  $\Ex\big[\sigma^i\big]=\bigg\{\kern-1ex
  \begin{array}{ll}1&i{\rm\ even}\\0&i{\rm\ odd}\end{array}$

  \ceq{}
  {=}
  {\sum^\infty_{i=0}\frac{(t^2/2)^{i}}{i!}}

  \ceq{}
  {=}
  {e^{t^2/2}.}
\end{proof}

\begin{void_def}[Second proof of Hoeffding's Lemma]\label{proof_2nd_Hoeffding}
  Recall that Hoeffding's Lemma claims that, if $a\le X\le b$, then
  
  \ceq{\hfill\Ex\Big[e^{t(X-\mu)}\Big]}
  {\le}
  {\exp{\Big(\frac{t^2d^2}8}\Big),}

  where $\Ex[X]=\mu$ and $d=b-a$.
  Here we prove a marginally weaker bound (2 in place of 8).

  Let $X'$ be an independent copy of $X$ (a.k.a.\@ ghost sample).
  In particular $\mu=\Ex(X')$.
  Then

  \ceq{\hfill\Ex\Big[e^{t(X-\mu)}\Big]}
  {=}
  {\Ex\Big[e^{t(X-\Ex[X'])}\Big]}


  \ceq{}
  {\le}
  {\Ex\Big[\Ex\big[e^{t(X-X')}\, |\, X\big]\Big]}\hfill by Jensen's inequality

  \ceq{}
  {\le}
  {\Ex\Big[e^{t(X-X')}\Big]}

  Let $\sigma$ be a random sign variable independent of $X,X'$.
  % That is, $\sigma\in\{-1,1\}$ with uniform distribution.
  % Assume also that $\sigma$ is independent of $X,X'$.
  Then $\sigma(X-X')$ has the same distribution of $X-X'$.

  \ceq{}
  {=}
  {\Ex\Big[e^{t\sigma(X-X')}\Big]}

  \ceq{}
  {=}
  {\Ex\bigg[\Ex\Big[e^{t\sigma(X-X')}\ |\ X,X'\Big]\bigg]}
  
  \ceq{}
  {\le}
  {\Ex\Big[e^{t^2(X-X')^2/2}\Big]}
  \hfill by Lemma~\ref{lem_sign}
      
  \ceq{}
  {\le}
  {e^{t^2d^2/2}}
  \hfill  because $|X-X'|\le d$.
  
This yields the bound above with 2 in place of 8.\QED
\end{void_def}
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
\section{Two Weak Laws of Large Numbers}\label{samples}
\def\medrel#1{\parbox[t]{5ex}{$\displaystyle\hfil #1$}}
\def\ceq#1#2#3{\parbox[t]{34ex}{$\displaystyle #1$}\medrel{#2}{$\displaystyle #3$}}
%\def\smallcirc{\mathord{\kern-.2ex\raisebox{.4ex}{$\scriptscriptstyle\circ$}}}

A \emph{sample\/} $s$ is a sequence $s_1,\dots,s_n$ of elements of $\U$.
Its length $|s|=n$ is also called \emph{size\/} or \emph{dimension.}
We write $\range(s)$ for the set $\{s_1,\dots,s_n\}$.
Note that this set may have cardinality $<n$.

To a sample $s$ of size $n$ we associate a finite probability measure on the subsets of $\U$ namely, for any event $A\subseteq\U$, we define the empirical frequence of $A$ given $s$

\ceq{\hfill\emph{$\displaystyle\Fr\big(s, A\big)$}}
{=}
{\frac1n\cdot \big|\big\{i\ :\ s_i\in A\big\}\big|.}

It is convenient to rewrite it using indicator functions

\ceq{\hfill \Fr(s,A)}
{=}
{\frac1n\sum^n_{i=1}\Indicator_{s_i\in A}.}

We are interested in the \textit{existence\/} of samples that approximate the probability within $\epsilon$.
Suppose that, for a given event $A$, we can prove that

(1)\hfil$\displaystyle\Pr \Big(s\in\U^n:\, \big|\Fr(s,A) - \Pr(A) \big|\ge\epsilon\Big)\ \ \le\ \ \textrm{some\_bound}(\epsilon,n)$

and that, for $n$ large enough, some\_bound$(\epsilon,n)<1$.
Then a sample of size $\le n$ that approximate the probability within $\epsilon$ is guaranteed to exit.

Random variables are convenient formalism to discuss these probabilities.
We say \emph{random element\/} of $\U$ for a random variables $S$ such that $\Pr(S\in A)=\Pr(A)$ for every $A\subseteq\U$.
A \emph{random sample\/} from $\U$ is a tuple $S=S_1,\dots,S_n$ of independent random elements of $\U$.
Then $\Indicator_{S_i\in A}=\Indicator_{A}\circ S_i$ is a Bernoulli random variable with probability of success $\Pr(A)$ and 

\ceq{\hfill \Fr(S,A)}
{=}
{\frac1n\sum^n_{i=1}\Indicator_{S_i\in A}}

is (up to the factor $1/n$) a binomial random variable.
This random variable is used below to find the bound in (1), in fact

\ceq{\hfill\Pr\Big(\big|\Fr(S,A) - \Pr(A)\big|\ge\epsilon\Big)}
  {}
  {}

equals the probability in (1) but is easier to estimate.

\begin{void_thm}[Weak Law of Large Numbers]\label{Weak_Law_1}
  For every event $A\subseteq\U$ and every tuple $S=S_1,\dots,S_n$ of independent random elements of $\U$
  
  \ceq{\hfill\Pr\Big(\big|\Fr(S,A) - \Pr(A)\big|\ge\epsilon\Big)}
  {\le}
  {\frac1{n\epsilon^2}.}
\end{void_thm}

\begin{proof}
  The random variable $\Fr(S,A)$ has expected value $\Pr(A)$ and variance $\le1/n$. 
  By Chebyshev's inequality we obtain 
  
  \ceq{\hfill\Pr\Big(\big|\Fr(S,A) - \Pr(A)\big|\ge\epsilon\Big)}
  {\le}
  {\frac1{n\epsilon^2}}

  which proves the theorem.
\end{proof}

Sometime we are interested in the minimal size of a sample that approximates the probability up to a given $\epsilon$.


\begin{corollary}\label{corl_wlln}
  Assume $\U$ is finite (of arbitrary cardinality, tough).
  For every $A\subseteq\U$ and every $\epsilon>0$ there is a sample $s$ of size

  \ceq{\hfill |s|}
      {=}
      {\left\lfloor\frac1{\epsilon^2}+1\right\rfloor} 
      
  such that

  \ceq{\hfill\Big|\Fr(s,A) - \Pr(A)\Big|}{<}{\epsilon.}
\end{corollary}

\begin{proof}
  By the Weak Law of Large Numbers above, a sample of size $n$ exists if

  \ceq{\hfill\frac1{n\epsilon^2}}
      {<}
      {1}
\end{proof}

In the following section we need a better bound for the Weak Law of Large Numbers.
This is obtained with a similar proof.

\begin{void_thm}[Weak Law of Large Numbers (with exponential bound)]\label{WLLN_exp}
  For every event $A\subseteq\U$ and every tuple $S=S_1,\dots,S_n$ of independent random elements of $\U$ 
  
  \ceq{\hfill\Pr\Big(\big|\Fr(S,A) - \Pr(A)\big|\ge\epsilon\Big)}
  {\le}
  {2\, e^{-2n\epsilon^2}.}
\end{void_thm}

\begin{proof}
  Define
  
  \ceq{\hfill M}
  {=}
  {\sum^n_{i=1}\Big(\Indicator_{S_i\in A}-\Ex[\Indicator_{S_i\in A}]\Big)}

  As $\Ex[\Indicator_{S_i\in A}]=\Pr(A)$, the inequality we have to prove can be rewritten as 

  \ceq{\hfill\Pr \Big(|M|\ge n\epsilon\Big)}
  {\le}
  {2\, e^{-2n\epsilon^2}}

  and this follows immediately from Hoeffding inequality.
  \end{proof}

Using the exponential bounds above, we can improve (by a constant factor) the size of the minimal sample size that approximates the probability obtained in Corollary~\ref{corl_wlln}.

\begin{corollary}
  For every $A\subseteq\U$ and every $\epsilon>0$ there is a sample $s$ of size $n$ where

  \ceq{\hfill n}
  {=}
  {\left\lfloor\frac{\log 2}{2\epsilon^2}+1\right\rfloor} 
      
  such that

  \ceq{\hfill\Big|\Fr(s,A) - \Pr(A) \Big|}
  {<}
  {\epsilon.}\QED
\end{corollary}
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
\section{A Uniform Law of Large Numbers}\label{uniform}

Throughout this section we work with a fixed family of definable subsets $\phi(\U\,;b)_{b\in\V}$ that are events of the sample space $\U,\Pr$.
It is convenient to introduce some abbreviations

\ceq{\hfill \Pr(b)}
{=}
{\Pr\Big(\phi(\U\,;b)\Big)}

\ceq{\hfill\Fr(s,b)}
{=}
{\Fr\Big(s,\phi(\U\,;b)\Big)}

\ceq{\hfill\Indicator_{s,b}}
{=}
{\Indicator_{\phi(s;b)}}

An \emph{$\epsilon$-approximation\/} is a sample $s$ such that

\ceq{\hfill\Big|\Fr(s,b) -\Pr(b)\Big|}
{<}
{\epsilon}
\hfill for every $b\in\V$.

We are interested in estimating the minimal size of an $\epsilon$-approximation.

The main theorem of this section is this famous result of Vapnik-Chervonenkis~\cite{VC}.


\def\medrel#1{\parbox[t]{5ex}{$\displaystyle\hfil #1$}}
\def\ceq#1#2#3{\parbox[t]{35ex}{$\displaystyle #1$}\medrel{#2}{$\displaystyle #3$}}

\begin{void_thm}[Vapnik-Chervonenkis Inequality]\label{VC_inequality}
  Let $\pi_\phi(n)$ be the shatter function of $\phi(\U\,;b)_{b\in\V}$.
  Let $S=S_1,\dots,S_n$ be a random sample from $\U$.
  Then

  \ceq{(1)\hfill\Pr\,\Big(\sup_{b\in\V}\big|\Fr(S,b) - \Pr(b)\big|\ge\epsilon\Big)}
  {\le}
  {8\,\pi_\phi(n)\exp\bigg(-\frac{n\epsilon^2}{32}\bigg).}
\end{void_thm}

N.B. Some technical measure-theoretical assumptions are necessary when $\phi(\U\,;b)_{b\in\V}$ is uncountable.
These have been omitted in the statement above, and will be discussed below.

\begin{proof}
  First of all notice that we can assume $n \ge \frac{18}{\epsilon^2}$, since otherwise the r.h.s of (1) would be $\ge 1$, making the statement trivially true. Let $S'=S'_1,\dots,S'_n$ be an independent copy of $S$ and $\sigma=\sigma_1,\dots,\sigma_n$ be a tuple of sign random variables independent from each other and from the previuos ones.
  Then
  
  \ceq{\hfill\Pr\bigg(\sup_{b\in\V}\big|\Fr(S,b) - \Fr(S',b)\big|\ge\frac\epsilon2\bigg)}
  {=}
  {\Pr\bigg(\sup_{b\in\V}\Big|\sum_{i=1}^n\Indicator_{S_i,b} - \Indicator_{S'_i,b}\Big|\ge\frac{n\epsilon}2\bigg)}
  \smallskip
  
  % \ceq{\hfill}
  % {=}
  % {\Pr\bigg(\Big|\sum_{i=1}^n\sigma_i\,\Indicator_{S_i,b} - \sum_{i=1}^n\sigma_i\,\Indicator_{S'_i,b}\Big|\ge\frac{n\epsilon}2\bigg)}
  % \smallskip

  \ceq{\hfill}
  {=}
  {\Pr\bigg(\sup_{b\in\V}\Big|\sum_{i=1}^n\sigma_i\big(\Indicator_{S_i,b} - \Indicator_{S'_i,b}\big)\Big|\ge\frac{n\epsilon}2\bigg)}
  \smallskip

  Then, by the triangular inequality

  \ceq{}
  {\le}
  {2\,\Pr\bigg(\sup_{b\in\V}\Big|\sum_{i=1}^n\sigma_i\,\Indicator_{S_i,b}\Big|\ge\frac{n\epsilon}4\bigg)}

    Let $s=s_1,\dots,s_n$ be a possible realization of $S$ then, by the independence of $\sigma$,\smallskip

  \ceq{(2)\hfill\Pr\bigg(\sup_{b\in\V}\Big|\sum_{i=1}^n\sigma_i\,\Indicator_{s_i,b}\Big|\ge\frac{n\epsilon}4\bigg)}
  {=}
  {\Pr\bigg(\sup_{b\in\V}\Big|\sum_{i=1}^n\sigma_i\,\Indicator_{S_i,b}\Big|\ge\frac{n\epsilon}4\ \big|\  S=s\bigg)}
  \smallskip

  Note that the l.h.s.\@ of (2) only depends on $\phi\big(\{s_1,\dots,s_n\}\,;b\big)$.
  Hence the $\sup_{b\in\V}(\cdot)$ on the l.h.s.\@ is actually a maximum among $m=\pi_\phi(n)$ events.
  Say, we can choose $b_1,\dots,b_m\in\V$ such that

  \ceq{(3)}
  {=}
  {\Pr\bigg(\max_{j\le m}\Big|\sum_{i=1}^n\sigma_i\,\Indicator_{s_i,b_j}\Big|\ge\frac{n\epsilon}4\bigg)}

  Note that, in general, for any real random variables $X_1,\dots,X_m$ we have

  \ceq{\hfill\Pr\Big(\max_{i\le m} X_i \ge \epsilon\Big)}
  {=}
  {\Pr\Big(\bigcup_{i=0}^m\ X_i \ge \epsilon\Big)}

  \ceq{}
  {\le}
  {\sum_{i=1}^m \Pr\big(X_i \ge\epsilon\big)}

  Hence, continuing from (3) we obtain

  \ceq{}
  {\le}
  {\sum_{j=1}^m \Pr\bigg(\Big|\sum_{i=1}^n\sigma_i\,\Indicator_{s_i,b_j}\Big|\ge\frac{n\epsilon}4\bigg).}

  \ceq{}
  {\le}
  {2\,\pi_\phi(n)\,\exp\bigg(-\frac{n\epsilon^2}{32}\bigg),}

  where the last inequality is obtained from Hoeffding's Inequality~\ref{Hoeffding_inequality}.
  In fact, Hoeffding's Inequality, applied to $X_i=\sigma_i\,\Indicator_{s_i,b}$ with $n\epsilon/4$ for $\epsilon$, yields

  \ceq{\hfill\Pr\bigg(\Big|\sum_{i=1}^n\sigma_i\,\Indicator_{s_i,b}\Big|\ge\frac{n\epsilon}4\bigg)}
  {\le}
  {2\,\exp\Big(-\frac{n\epsilon^2}{32}\Big).}

  Hence, by (2), we can conclude that
  
  \ceq{\hfill\Pr\bigg(\sup_{b\in\V}\Big|\sum_{i=1}^n\sigma_i\,\Indicator_{S_i,b}\Big|\ge\frac{n\epsilon}4\bigg)}
  {\le}
  {2\,\pi_\phi(n)\,\exp\bigg(-\frac{n\epsilon^2}{32}\bigg),}
  \smallskip
  
  
  Now let $X_0\subseteq \U^n$ be the set of all $s = s_1,\dots,s_n$ s.t. $\Pr(\sup_{b\in\V}|\Fr(s,b) - \Fr(S',b)|\ge\frac{\epsilon}2) > \frac{1}{2}$.
  By what we've said we have
 $$\Pr\big(S \in X_0\big)\Pr\bigg(\sup_{b\in\V}\Big|\Fr(S,b) - \Fr(S',b)\Big| \ge\frac{\epsilon}2 \ \big|\ S \in X_0 \bigg)
  \le
  4\,\pi_\phi(n)\,\exp\bigg(-\frac{n\epsilon^2}{32}\bigg)$$
 
  by independence of $S,S'$ and by definition of $X_0$ we get 
  
   \ceq{\hfill\Pr\big(S \in X_0\big)}
  {\le}
  {8\,\pi_\phi(n)\,\exp\bigg(-\frac{n\epsilon^2}{32}\bigg),}
  
  Fix $s \in \U^n \setminus X_0$ and $b \in \V$, by the first Weak Law of Large Numbers~\ref{Weak_Law_1} and our assumptions on $n$ we have
  
  \ceq{\hfill\Pr\Big(\big|\Fr(S',b) - \Pr(b)\big|<\frac{\epsilon}{3}\Big)}
  {>}
  {\frac{1}{2}.}
  
  This, combined with the fact that $\Pr(\sup_{b\in\V}|\Fr(s,b) - \Fr(S',b)|<\frac{\epsilon}2) > \frac{1}{2}$, allows us to say that there exist a tuple $t \in \U^n$ s.t.
  \begin{itemize}
      \item $\big|\Fr(s,b)-\Fr(t,b)\big| < \frac{\epsilon}{2}$
      \item $\big|\Fr(t,b) - \Pr(b)\big|<\frac{\epsilon}{3}$
  \end{itemize}
  
  and therefore by triangular inequality $\big|\Fr(s,b) - \Pr(b)\big|< \frac{5\epsilon}6$. As $b \in \V$ was arbitrary we have that for all $s \in \U^n\setminus X_0$, $\sup_{b\in\V}|\Fr(s,b) - \Pr(b)|< \epsilon$. So
  
  \ceq{\hfill\Pr\,\Big(\sup_{b\in\V}\big|\Fr(S,b) - \Pr(b)\big|\ge\epsilon\Big)}
  {\le}
  {\Pr\big(S \in X_0\big)}
  
  and this concludes the proof.
\end{proof}

\begin{corollary}
  Let $\phi(\U\,;b)_{b\in\V}$ have finite \vc-dimension. 
  For every every $\epsilon>0$ there is a finite sample $s$ such that

  \ceq{\hfill\Big|\Fr(s,b) - \Pr(b) \Big|}{<}{\epsilon}\hfill for every $b\in\V$.
\end{corollary}
\begin{proof}
  It suffices to require that $n=|s|=|S|$ is large enough to guarantee 
  
  \ceq{\hfill\Pr\,\Big(\big|\Fr(S,b) - \Pr(b)\big|\ge\epsilon\Big)}
  {<}
  {1}\hfill for every $b\in\V$.

  By the Vapnik-Chervonenkis inequality~\ref{VC_inequality}, it suffices that

  \ceq{(3)\hfill6\;\pi_\phi(n)\,\exp\bigg(-\frac{n\epsilon^2}{32}\bigg)}
  {<}
  {1}

  By the Sauer-Shelah Lemma~\ref{lem_sauer}, $\pi_\phi(n)$ grows polynomially.
  Hence the inequality holds for $n$ large enough.
\end{proof}

The corollary above is sufficient for our intended applications.
For completeness, the following proposition gives an explicit bound.

\begin{proposition}\label{prop_vc_sample}
  There is a sample $s$ as in the corollary above of size
  
  \ceq{\hfill n}
  {\le}
  {c\,\frac{k}{\epsilon^2}\log\frac{k}{\epsilon}}
  
  where $c$ is an absolute constant and $k$ is the \vc-dimension of $\phi(\U\,;b)_{b\in\V}$.
\end{proposition}

\begin{proof}[Proofsketch]
  By (3) in the proof above and the inequality proved after the Sauer-Shelah Lemma~\ref{lem_sauer} it suffices that $n=|s|$ satisfies
  
  \ceq{\hfill\log 6+k\,\log(n+1)}
  {<}
  {\frac{n\epsilon^2}{32},}
  
  which is the case if $n$ satisfies the following inequality 

  \ceq{\hfill c'\,\frac{ k}{\epsilon^2}}
  {<}
  {\frac{n}{\log n},}

  for some absolute constant $c'$.
  Finally, the latter inequality is satisfied if 

  \ceq{\hfill c''\,\frac{k}{\epsilon^2}\log\frac{k}{\epsilon^2}}
  {<}
  {n}

  for some absolute constant $c''$, see the exercise below.
\end{proof}

\begin{exercise}
  Prove that for all $x,y>1$

  \ceq{\hfill 2x\log x <y}
  {\IMP}
  {x< \frac{y}{\log y}}\QED
  
  %Hint: it suffices to prove that $2x\log x <y$  implies $\log x^2<\log y$.
\end{exercise}



% In fact the theorem is still true if $X$ is infinite, but we have to add the following measurability assumptions:

% -- for each $n$, the function $X^n \to \mathbb R$, $(x_1,\dots,x_n)\mapsto \sup_{S\in \mathcal S} |\Av(x_1,\dots,x_n)-\mu(S)|$ is measurable;

% -- for each $n$, the function $X^{2n} \to \mathbb R$, $(x_1,\dots,x_n,x'_1,\dots,x'_n) \mapsto \sup_{S\in \mathcal S} \delta(\bar x,\bar x';S)$ is measurable.

% The first condition implies the other two when the family $\mathcal S$ is countable, and of course they always hold when $X$ is finite. The proof then goes through.

% To see how the second and third hypothesis might fail, consider the case of $X=\omega_1$. Let $\mathcal B$ be the $\sigma$-algebra generated by the intervals. Let $\mu$ be defined on $\mathcal B$ by $\mu(A)=1$ if $A$ contains an end segment of $X$ and $\mu(A)=0$ otherwise. Note that this defines a $\sigma$-additive measure on $(X,\mathcal B)$. Let $\mathcal S$ to be the family of intervals of $X$. It has VC-dimension 2. We leave it to the reader to check that the VC-theorem does not hold for $\mathcal S$. (In view of Corollary \ref{cor_vcthm} it is enough to check that there are no $\epsilon$-nets, for $\epsilon<1$).
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
\section{A Uniform Law of Large Numbers, again}\label{uniform2}

We prove a second version of the Vapnik-Chervonenkis Inequality.
Which, I conjecture, is due to Devroye and Lugosi~\cite{DL}.
 
\begin{void_thm}[Vapnik-Chervonenkis Inequality (2)]\label{DL_inequality}
  Let $\pi_\phi(n)$ be the shatter function of $\phi(\U\,;b)_{b\in\V}$.
  Let $S=S_1,\dots,S_n$ be a random sample from $\U$.
  Then, for every $b\in\V$

  \ceq{\hfill\Ex\,\Big|\Fr(S,b) - \Pr(b)\Big|}
  {\le}
  {2\sqrt{\frac{\log\;\big(2\,\pi_\phi(n)\big)}{n}}}.\QED
\end{void_thm}

The same caveat on measurability apply as for Inequality~\ref{VC_inequality}.

We note that the bound is not optimal, using a clever techniques called \textit{chaining}, Dudley could prove that

\ceq{\hfill\Ex\,\Big|\Fr(S,b) - \Pr(b)\Big|}
{<}
{c\sqrt{\frac{k}{n}},}

where $k$ is the \vc-dimension and $c$ is absolute constant.

Before embarking in the proof of the theorem above, we prove the following (easy, although mysterious) lemma, which also has independent interest.

\begin{lemma}\label{lem_mistero}
  Let $X_1,\dots,X_m$ be real valued random variables.
  Let $c$ be such that 

  \ceq{\hfill\Ex\big[e^{tX_i}\big]}
  {\le}
  {e^{c^2t^2/2}}
  \hfill for every $i\le m$ and every $t>0$.

  Then 

  \ceq{\hfill\Ex\big[\max_{i\le m} X_i\big]}
  {\le}
  {c\sqrt{2\log m}}.

  If in addition

  \ceq{\hfill\Ex\big[e^{-tX_i}\big]}
  {\le}
  {e^{c^2t^2/2}}
  \hfill for every $i\le m$ and every $t>0$,

  then 

  \ceq{\hfill\Ex\big[\max_{i\le m} |X_i|\big]}
  {\le}
  {c\sqrt{2\log(2m)}}.
\end{lemma}

\begin{proof}
  By Jensen's inequality,

  \ceq{\hfill\exp\Big(t\cdot \Ex\big[\max_{i\le m} X_i\,\big]\Big)}
  {\le}
  {\Ex\Big[\exp\!\big(\max_{i\le m} t X_i\,\big)\Big]}

  \ceq{}
  {=}
  {\Ex\Big[\max_{i\le m} e^{t X_i}\Big]}

  \ceq{}
  {\le}
  {\Ex\Big[\sum_{i\le m} e^{t X_i}\Big]}

  \ceq{}
  {=}
  {\sum_{i\le m}\Ex\big[ e^{t X_i}\big]}

  \ceq{}
  {\le}
  {m\,e^{c^2t^2/2}}

  Taking the logarithm of both sides and replacing $t$ with \smash{$\dfrac{\sqrt{2\log m}}{c}$}, we obtain the first inequality of the lemma.

  To prove the second inequality, apply the first one to $X_1,\dots,X_m, -X_1,\dots,-X_m$.
  (N.B. note that independence is not assumed.)
\end{proof}

\textbf{Proof of the Vapnik-Chervonenkis inequality}
  Let $S'=S'_1,\dots,S'_n$ be an independent copy of $S$.
  We claim that

  \ceq{(1)\hfill\Ex\Big[\sup_{b\in\V}\big|\Fr(S,b) - \Pr(b)\big|\Big]}
  {\le}
  {\Ex\Big[\sup_{b\in\V}\big|\Fr(S,b) - \Fr(S',b)\big|\Big]}

  In fact,

  \ceq{\hfill\Fr(S,b) - \Pr(b)}
  {=}
  {\Fr(S,b) - \Ex\big[\Fr(S',b)\big]}

  \ceq{}
  {=}
  {\Ex\Big[\Fr(S,b) - \Fr(S',b)\ \big|\ S\Big].}

  Now, apply Jensen's inequality to the absolute value function, then use that 

  \ceq{(2)\hfill\sup_{b\in\V}\Ex\big[\cdots\big]}
  {\le}
  {\Ex\big[\sup_{b\in\V}(\cdots)\big].}

  Write $\Indicator_b$ for the indicator function of $\phi(\U\,;b)$.
  Then

  \ceq{\hfill\big|\Fr(S,b) - \Fr(S',b)\big|}
  {=}
  {\frac1n\bigg|\sum^n_{i=1} \Big(\Indicator_{S_i,b} -  \Indicator_{S'_i,b}\Big)\bigg|}

  Let $\sigma=\sigma_1,\dots,\sigma_n$ be a tuple of independent sign random variable.
  The random variable $\Indicator_{S_i,b} -  \Indicator_{S'_i,b}$ has the same distribution of $\sigma_i\big(\Indicator_{S_i,b} -  \Indicator_{S'_i,b}\big)$ hence

  \ceq{}
  {=}
  {\frac1n\Ex\bigg|\sum^n_{i=1} \sigma_i\Big(\Indicator_{S_i,b} -  \Indicator_{S'_i,b}\Big)\ \Big|\ S,S'\bigg|}

  Inserting this into (1) we obtain

  \ceq{\hfill\Ex\Big[\sup_{b\in\V}\big|\Fr(S,b) - \Pr(b)\big|\Big]}
  {\le}
  {\frac1n\,\Ex\bigg[\sup_{b\in\V}\Ex\bigg|\sum^n_{i=1} \sigma_i\Big(\Indicator_{S_i,b} -  \Indicator_{S'_i,b}\Big)\ \Big|\ S,S'\bigg|\bigg]}

  Let $s,s'$ be a generic realization of $S,S'$

  \ceq{}
  {\le}
  {\frac1n\,\sup_{s,s'}\,\sup_{b\in\V}\Ex\bigg|\sum^n_{i=1} \sigma_i\big(\Indicator_{s_i,b} -  \Indicator_{s'_i,b}\big)\bigg|}

  and, by what remarked in (2)

  \ceq{}
  {\le}
  {\frac1n\,\sup_{s,s'}\Ex\bigg[\sup_{b\in\V}\bigg|\sum^n_{i=1} \sigma_i\big(\Indicator_{s_i,b} -  \Indicator_{s'_i,b}\big)\bigg|\bigg]}

  Observe that once $s,s'$ is fixed, $\sup_{b\in\V}$ is actually a maximum among $\pi_\phi(2n)$ sets, in fact, $\pi_\phi(2n)$ is the number of definable subsets of $A=\{s_1,\dots,s_n,s'_1,\dots,s'_n\}$.
  Then, by Lemma~\ref{lem_mistero} (the second inequality, with $m =\pi_\phi(2n)$ and $i$ ranging over the definable subsets of $A$), for an appropriate constant $c$,

  \ceq{}
  {\le}
  {\frac1n\,\sup_{s,s'}c\sqrt{2\log\big( 2\,\pi_\phi(2n)\big)}.}

  As the r.h.s.\@ does not depend on $s,s'$,

  \ceq{}
  {\le}
  {\frac{c}n\sqrt{2\log\big(2\,\pi_\phi(2n)\big)}}

  Finally, as $\pi_\phi(2n)\le\pi_\phi(n)^2$,

  \ceq{}
  {\le}
  {\frac{2c}n\sqrt{\log\big(2\,\pi_\phi(n)\big)}}
  
  The Vapnik-Chervonenkis inequality is proved if we can show the assumption of Lemma~\ref{lem_mistero} holds with $c=\sqrt{n}$.\smallskip

  \ceq{\hfill\Ex\bigg[\exp\bigg(t\sum^n_{i=1} \sigma_i\big(\Indicator_{s_i,b} -  \Indicator_{s'_i,b}\big)\bigg)\bigg]}
  {=}
  {\prod^n_{i=1} \Ex\bigg[\exp\Big(t\,\sigma_i\big(\Indicator_{s_i,b} -  \Indicator_{s'_i,b}\big)\Big)\bigg]}
  \smallskip

  As $\sigma_i\big(\Indicator_{s_i,b} -  \Indicator_{s'_i,b}\big)$ takes values in $\{-1,1\}$ with mean $0$, by Lemma~\ref{lem_sign}

  \ceq{}
  {\le}
  {e^{nt^2/2}}

  and the same holds for $-\sigma_i\big(\Indicator_{s_i,b} -  \Indicator_{s'_i,b}\big)$.%
\QED

As an application we prove the Glivenko-Cantelli Theorem, an important theorem of mathematical statistics.
The theorem says that the empirical cumulative distribution function converges uniformly to the true one.
We prove an informative variant which gives the rate of convergence (though, this is not optimal).

\begin{void_thm}[Glivenko-Cantelli Theorem]
  Let $X=X_1,\dots,X_n$ be i.i.d.\@ random variables.
  Let $F(z)=\Pr(X_i\le z)$ be their common cumulative distribution function.
  Let $F_{\rm e}(x)$ be the empirical cumulative distribution function, that is,

  \ceq{\hfill F_{\rm e}(X,z)}
  {=}
  {\frac1n\sum^n_{i=1}\Indicator_{X_i\le z}.}

  Then, for every $z\in\RR$,

  \ceq{\hfill\Ex\,\Big|F_{\rm e}(X,z) - F(z)\Big|}
  {\le}
  {2\,\sqrt{\frac{\log\big(2n+2\big)}{n}}}.\QED
\end{void_thm}

\begin{proof}
  Take $\U=\V=\RR$ and $\phi(x\,;z) = x\le z$.
  Assign to $\phi(\U\,;b)=(-\infty,b]$ probability $\Pr(X_i\le b)$.
  Then, if $S=S_1,\dots,S_n$ is a random sample from $\U$

  \ceq{\hfill\Ex\,\Big|F_{\rm e}(X,z) - F(z)\Big|}
  {=}
  {\Ex\Big|\Fr(S,b) - \Pr(b)\Big|}

  hence, by the Vapnik-Chervonenkis inequality~\ref{VC_inequality}

  \ceq{}
  {\le}
  {\sqrt{2\frac{\log\;\big(2\,\pi_\phi(n)\big)}{n}}}.
  
  It is clear that $\phi(\U\,;b)_{b\in\V}$ has \vc-dimension $1$.
  By the Sauer-Shelah Lemma~\ref{lem_sauer} and the inequalities proved thereafter, $\pi_\phi(n)\le n+1$.
  The theorem follows.
\end{proof}
\end{document}
